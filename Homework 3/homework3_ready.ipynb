{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - classifiers\n",
    "\n",
    "In this homework we will use 7 algorithms to classify data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite knowledge\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Precision &= \\frac{\\text{True positives}}{\\text{True positives} + \\text{False positives}} \\\\\n",
    "    Recall &= \\frac{\\text{True positives}}{\\text{True positives} + \\text{False negatives}} \\\\\n",
    "    F1 &= 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Note**: This is mostly for learning purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    os.environ[\"R_HOME\"] = \"C:\\Program Files\\R\\R-4.3.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OneHotToNumericPipeline(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, base):\n",
    "        self.base = base\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return np.argmax(self.base.predict(X), axis=1)\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return np.array(list(range(len(self.base.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumericOnlyPipeline(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, base):\n",
    "        self.base = base\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.base.fit(X, np.argmax(y, axis=1))\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        predicted = self.base.predict(X)\n",
    "\n",
    "        encoded_arr = np.zeros((predicted.size, 6 + 1), dtype=int)\n",
    "        encoded_arr[np.arange(predicted.size), predicted] = 1\n",
    "\n",
    "        return encoded_arr\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        n_classes = len(self.base.classes_)\n",
    "        return [np.array([False, True]) for it in range(n_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset-specific dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_VARIABLE = \"NObeyesdad\"\n",
    "NUMERICAL_VARIABLES = [\"Age\", \"Height\", \"Weight\", \"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
    "CATEGORICAL_VARIABLES_NO_LABEL = [\n",
    "    \"FAVC\",\n",
    "    \"CAEC\",\n",
    "    \"CALC\",\n",
    "    \"SCC\",\n",
    "    \"MTRANS\",\n",
    "    \"Gender\",\n",
    "    \"family_history_with_overweight\",\n",
    "    \"SMOKE\",\n",
    "]\n",
    "CATEGORICAL_VARIABLES = [\n",
    "    *CATEGORICAL_VARIABLES_NO_LABEL,\n",
    "    LABEL_VARIABLE,\n",
    "]\n",
    "ALL_VARIABLES_NO_LABEL = [*NUMERICAL_VARIABLES, *CATEGORICAL_VARIABLES_NO_LABEL]\n",
    "ALL_VARIABLES = [*NUMERICAL_VARIABLES, *CATEGORICAL_VARIABLES]\n",
    "LABEL_DICTIONARY = {\n",
    "    \"Age\": \"Age\",\n",
    "    \"Height\": \"Height (cm)\",\n",
    "    \"Weight\": \"Weight (kg)\",\n",
    "    \"FCVC\": \" Frequency of consumption of vegetables (times per day)\",\n",
    "    \"NCP\": \"Number of main meals\",\n",
    "    \"CH2O\": \"Consumption of water daily (Liters)\",\n",
    "    \"FAF\": \"Physical activity frequency (times per day)\",\n",
    "    \"TUE\": \"Time using technology devices (hours)\",\n",
    "    \"FAVC\": \"Frequent consumption of high caloric food\",\n",
    "    \"CAEC\": \"Consumption of food between meals\",\n",
    "    \"CALC\": \"Consumption of alcohol\",\n",
    "    \"SCC\": \"Calories consumption monitoring\",\n",
    "    \"MTRANS\": \"Transportation used\",\n",
    "    \"Gender\": \"Gender\",\n",
    "    \"family_history_with_overweight\": \"Family member suffered or suffers from overweight\",\n",
    "    \"SMOKE\": \"Smoker or not\",\n",
    "    \"NObeyesdad\": \"Obesity level\",\n",
    "}\n",
    "\n",
    "T = t.TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Person:\n",
    "    Gender: str\n",
    "    Age: np.int32\n",
    "    Height: np.float32\n",
    "    Weight: np.float32\n",
    "    family_history_with_overweight: str\n",
    "    FAVC: str\n",
    "    FCVC: np.float32\n",
    "    NCP: np.float32\n",
    "    CAEC: str\n",
    "    SMOKE: str\n",
    "    CH2O: np.float32\n",
    "    SCC: str\n",
    "    FAF: np.float32\n",
    "    TUE: np.float32\n",
    "    CALC: str\n",
    "    MTRANS: str\n",
    "    NObeyesdad: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        Gender: str,\n",
    "        Age: str,\n",
    "        Height: str,\n",
    "        Weight: str,\n",
    "        family_history_with_overweight: str,\n",
    "        FAVC: str,\n",
    "        FCVC: str,\n",
    "        NCP: str,\n",
    "        CAEC: str,\n",
    "        SMOKE: str,\n",
    "        CH2O: str,\n",
    "        SCC: str,\n",
    "        FAF: str,\n",
    "        TUE: str,\n",
    "        CALC: str,\n",
    "        MTRANS: str,\n",
    "        NObeyesdad: str,\n",
    "    ):\n",
    "        self.Gender = Gender\n",
    "        self.Age = np.float32(Age)\n",
    "        self.Height = np.float32(Height)\n",
    "        self.Weight = np.float32(Weight)\n",
    "        self.family_history_with_overweight = family_history_with_overweight\n",
    "        self.FAVC = FAVC\n",
    "        self.FCVC = np.float32(FCVC)\n",
    "        self.NCP = np.float32(NCP)\n",
    "        self.CAEC = CAEC\n",
    "        self.SMOKE = SMOKE\n",
    "        self.CH2O = np.float32(CH2O)\n",
    "        self.SCC = SCC\n",
    "        self.FAF = np.float32(FAF)\n",
    "        self.TUE = np.float32(TUE)\n",
    "        self.CALC = CALC\n",
    "        self.MTRANS = MTRANS\n",
    "        self.NObeyesdad = NObeyesdad\n",
    "\n",
    "    def __str__(self):\n",
    "        return vars(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(vars(self))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return vars(self)\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self, path_to_csv: str):\n",
    "        self.path_to_csv = path_to_csv\n",
    "\n",
    "    def load_as_obj_list(self) -> list[Person]:\n",
    "        with open(self.path_to_csv) as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            return [Person(**row) for row in csv_reader]\n",
    "\n",
    "    def process_dataframe_one_hot(\n",
    "        self, arg_dataset: pd.DataFrame\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "        result_features_dataframe = pd.DataFrame()\n",
    "        variable_to_indexes = dict()\n",
    "\n",
    "        old_features_dataframe_len = len(result_features_dataframe.columns)\n",
    "\n",
    "        for variable in NUMERICAL_VARIABLES:\n",
    "            result_features_dataframe = pd.concat(\n",
    "                [result_features_dataframe, arg_dataset[variable]], axis=1\n",
    "            )\n",
    "\n",
    "            variable_to_indexes[variable] = tuple(\n",
    "                range(\n",
    "                    old_features_dataframe_len, len(result_features_dataframe.columns)\n",
    "                )\n",
    "            )\n",
    "            old_features_dataframe_len = len(result_features_dataframe.columns)\n",
    "\n",
    "        for variable in CATEGORICAL_VARIABLES_NO_LABEL:\n",
    "            result_with_dummies = pd.get_dummies(arg_dataset[variable]).astype(float)\n",
    "            result_features_dataframe = pd.concat(\n",
    "                [result_features_dataframe, result_with_dummies], axis=1\n",
    "            )\n",
    "\n",
    "            variable_to_indexes[variable] = tuple(\n",
    "                range(\n",
    "                    old_features_dataframe_len, len(result_features_dataframe.columns)\n",
    "                )\n",
    "            )\n",
    "            old_features_dataframe_len = len(result_features_dataframe.columns)\n",
    "\n",
    "        result_labels_dataframe = pd.get_dummies(arg_dataset[LABEL_VARIABLE])\n",
    "        return result_features_dataframe, result_labels_dataframe, variable_to_indexes\n",
    "\n",
    "    def process_dataframe_numeric(\n",
    "        self, arg_dataset: pd.DataFrame\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        result_features_dataframe = pd.DataFrame()\n",
    "\n",
    "        for variable in NUMERICAL_VARIABLES:\n",
    "            result_features_dataframe = pd.concat(\n",
    "                [result_features_dataframe, arg_dataset[variable]], axis=1\n",
    "            )\n",
    "\n",
    "        for variable in CATEGORICAL_VARIABLES_NO_LABEL:\n",
    "            result_with_dummies = pd.DataFrame(\n",
    "                {variable: LabelEncoder().fit_transform(arg_dataset[variable]).tolist()}\n",
    "            )\n",
    "            result_features_dataframe = pd.concat(\n",
    "                [result_features_dataframe, result_with_dummies], axis=1\n",
    "            )\n",
    "\n",
    "        result_labels_dataframe = pd.DataFrame(\n",
    "            {\n",
    "                LABEL_VARIABLE: LabelEncoder()\n",
    "                .fit_transform(arg_dataset[LABEL_VARIABLE])\n",
    "                .tolist()\n",
    "            }\n",
    "        )\n",
    "        return result_features_dataframe, result_labels_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(\"data/ObesityDataSet.csv\")\n",
    "dataset_obj_list = dataset_manager.load_as_obj_list()\n",
    "dataset_dataframe = pd.DataFrame.from_records(\n",
    "    data=[vars(entry) for entry in dataset_obj_list]\n",
    ")\n",
    "dataset_output_classes = len(dataset_dataframe[LABEL_VARIABLE].unique())\n",
    "all_output_classes = dataset_dataframe[LABEL_VARIABLE].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use information gain to select best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>1.248984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.582005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height</th>\n",
       "      <td>0.425246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCVC</th>\n",
       "      <td>0.406311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH2O</th>\n",
       "      <td>0.305902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TUE</th>\n",
       "      <td>0.286994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAF</th>\n",
       "      <td>0.283382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCP</th>\n",
       "      <td>0.260715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>0.193117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAEC</th>\n",
       "      <td>0.175757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <td>0.149817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MTRANS</th>\n",
       "      <td>0.089742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALC</th>\n",
       "      <td>0.086610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAVC</th>\n",
       "      <td>0.051135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCC</th>\n",
       "      <td>0.026996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOKE</th>\n",
       "      <td>0.018079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_numeric, y_numeric = dataset_manager.process_dataframe_numeric(dataset_dataframe)\n",
    "\n",
    "x_numeric, y_numeric = x_numeric.astype(float), y_numeric.astype(float)\n",
    "\n",
    "mutual_info = pd.DataFrame(\n",
    "    mutual_info_classif(x_numeric, y_numeric).reshape(-1, 1),\n",
    "    columns=[\"Coefficient\"],\n",
    "    index=x_numeric.columns,\n",
    ").sort_values(by=[\"Coefficient\"], ascending=False)\n",
    "\n",
    "HTML(mutual_info.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAIN_SELECTED_FEATURES = [\"Weight\"]\n",
    "\n",
    "SECONDARY_SELECTED_FEATURES = [\n",
    "    \"Age\",\n",
    "    \"Height\",\n",
    "    \"FCVC\",\n",
    "    \"FAF\",\n",
    "    \"CH2O\",\n",
    "    \"TUE\",\n",
    "    \"NCP\",\n",
    "    \"Gender\",\n",
    "]\n",
    "\n",
    "SELECTED_FEATURES = MAIN_SELECTED_FEATURES + SECONDARY_SELECTED_FEATURES\n",
    "\n",
    "\n",
    "def make_feature_combinations(\n",
    "    main_features: t.List[str], secondary_features: t.List[str]\n",
    ") -> list[list[str]]:\n",
    "    combinations = []\n",
    "    for item in secondary_features:\n",
    "        combinations.append(main_features + [item])\n",
    "    return combinations\n",
    "\n",
    "\n",
    "SELECTED_FEATURES_COMBINATIONS = make_feature_combinations(\n",
    "    MAIN_SELECTED_FEATURES, SECONDARY_SELECTED_FEATURES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KFoldCrossValidation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_dataframe: pd.DataFrame,\n",
    "        labels_dataframe: pd.DataFrame,\n",
    "        feature_to_indexes: dict,\n",
    "    ):\n",
    "        self.k_fold_data = self.__get_data_splitting_dict(\n",
    "            features_dataframe, labels_dataframe\n",
    "        )\n",
    "\n",
    "        self.feature_to_indexes = feature_to_indexes\n",
    "\n",
    "    def __get_data_splitting_dict(\n",
    "        self, arg_features_dataframe: pd.DataFrame, arg_labels_dataframe: pd.DataFrame\n",
    "    ) -> list[tuple[np.array, np.array]]:\n",
    "        data_splitting_dict = dict[str, tuple[np.array, np.array]]()\n",
    "\n",
    "        for column in arg_labels_dataframe.columns:\n",
    "            indexes = arg_labels_dataframe.index[arg_labels_dataframe[column] == True]\n",
    "\n",
    "            current_feature_rows = arg_features_dataframe.iloc[indexes].to_numpy()\n",
    "            current_label_rows = arg_labels_dataframe.iloc[indexes].to_numpy()\n",
    "\n",
    "            data_splitting_dict[column] = (current_feature_rows, current_label_rows)\n",
    "\n",
    "        k_fold_data: list[tuple[np.array, np.array]] = []\n",
    "\n",
    "        for _, (\n",
    "            current_feature_rows,\n",
    "            current_label_rows,\n",
    "        ) in data_splitting_dict.items():\n",
    "            for it, (features, labels) in enumerate(\n",
    "                zip(\n",
    "                    np.array_split(current_feature_rows, 5),\n",
    "                    np.array_split(current_label_rows, 5),\n",
    "                )\n",
    "            ):\n",
    "\n",
    "                if it >= len(k_fold_data):\n",
    "                    k_fold_data.append((features, labels))\n",
    "                else:\n",
    "                    k_fold_data[it] = np.vstack(\n",
    "                        (k_fold_data[it][0], features)\n",
    "                    ), np.vstack((k_fold_data[it][1], labels))\n",
    "\n",
    "        for it in range(5):\n",
    "            permutation = np.random.permutation(len(k_fold_data[it][0]))\n",
    "            k_fold_data[it] = (\n",
    "                k_fold_data[it][0][permutation],\n",
    "                k_fold_data[it][1][permutation],\n",
    "            )\n",
    "\n",
    "        assert sum([np.shape(k_fold_data[it][0])[0] for it in range(5)]) == len(\n",
    "            dataset_dataframe\n",
    "        )\n",
    "        return k_fold_data\n",
    "\n",
    "    def get_train_val_split(\n",
    "        self, idx: int, selected_features: list[str]\n",
    "    ) -> tuple[tuple[np.array, np.array], tuple[np.array, np.array]]:\n",
    "        train_split = None\n",
    "        val_split = None\n",
    "\n",
    "        for ite, fold in enumerate(self.k_fold_data):\n",
    "            if idx == ite:\n",
    "                val_split = fold\n",
    "            else:\n",
    "                if train_split is None:\n",
    "                    train_split = fold\n",
    "                else:\n",
    "                    train_split = np.vstack((train_split[0], fold[0])), np.vstack(\n",
    "                        (train_split[1], fold[1])\n",
    "                    )\n",
    "\n",
    "        return self.trim_to_selected_features(\n",
    "            train_split, selected_features\n",
    "        ), self.trim_to_selected_features(val_split, selected_features)\n",
    "\n",
    "    def get_train_val_full(self, selected_features: list[str]):\n",
    "        return self.get_train_val_split(10, selected_features)[0]\n",
    "\n",
    "    def trim_to_selected_features(\n",
    "        self, dataset: tuple[np.array, np.array], selected_features: list[str]\n",
    "    ) -> tuple[np.array, np.array]:\n",
    "\n",
    "        if dataset is None:\n",
    "            return None\n",
    "\n",
    "        selected_indexes = []\n",
    "        for sel_feature in selected_features:\n",
    "            selected_indexes += self.feature_to_indexes[sel_feature]\n",
    "        return dataset[0][:, selected_indexes], dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    def __init__(self, labels: tuple[str]):\n",
    "        self.y_true: Optional[pd.DataFrame] = None\n",
    "        self.y_hat: Optional[pd.DataFrame] = None\n",
    "        self.labels = labels\n",
    "\n",
    "    def register_predictions(\n",
    "        self,\n",
    "        arg_y_true: t.Union[pd.DataFrame, np.array],\n",
    "        arg_y_hat: t.Union[pd.DataFrame, np.array],\n",
    "    ):\n",
    "        assert (self.y_true is None) == (self.y_hat is None)\n",
    "\n",
    "        if isinstance(arg_y_true, np.ndarray):\n",
    "            assert isinstance(arg_y_hat, np.ndarray)\n",
    "            arg_y_true = pd.DataFrame(arg_y_true)\n",
    "            arg_y_hat = pd.DataFrame(arg_y_hat)\n",
    "\n",
    "        if self.y_true is None:\n",
    "            self.y_true = arg_y_true\n",
    "            self.y_hat = arg_y_hat\n",
    "        else:\n",
    "            self.y_true = pd.concat([self.y_true, arg_y_true], axis=0)\n",
    "            self.y_hat = pd.concat([self.y_hat, arg_y_hat], axis=0)\n",
    "\n",
    "    def plot_confusion_matrix(self):\n",
    "        cm = confusion_matrix(\n",
    "            np.argmax(self.y_true, axis=1),\n",
    "            np.argmax(self.y_hat, axis=1),\n",
    "            labels=[0, 1, 2, 3, 4, 5, 6],\n",
    "        )\n",
    "        disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm, display_labels=all_output_classes\n",
    "        )\n",
    "        disp.plot()\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    def compute_accuracy_overall(self) -> float:\n",
    "        return accuracy_score(self.y_true, self.y_hat)\n",
    "\n",
    "    def compute_precision_foreach(self) -> np.array:\n",
    "        return precision_score(self.y_true, self.y_hat, average=None)\n",
    "\n",
    "    def compute_recall_foreach(self) -> np.array:\n",
    "        return recall_score(self.y_true, self.y_hat, average=None)\n",
    "\n",
    "    def compute_f1_foreach(self) -> np.array:\n",
    "        return f1_score(self.y_true, self.y_hat, average=None)\n",
    "\n",
    "    def compute_metrics(self) -> pd.DataFrame:\n",
    "        result = pd.DataFrame(\n",
    "            {\n",
    "                \"Precision score\": [np.average(self.compute_precision_foreach())],\n",
    "                \"Recall score\": [np.average(self.compute_recall_foreach())],\n",
    "                \"F1 score\": [np.average(self.compute_f1_foreach())],\n",
    "            }\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotManager:\n",
    "    def __init__(self, cols: int, samples: int) -> None:\n",
    "        self.rows = samples // cols + 1\n",
    "        self.cols = cols\n",
    "        self.samples = samples\n",
    "        self.index = 0\n",
    "        self.fig = None\n",
    "        self.axs = []\n",
    "\n",
    "    def init(self):\n",
    "        fig = plt.figure()\n",
    "        fig, axs = plt.subplots(self.rows, self.cols)\n",
    "        plt.tight_layout(h_pad=2, w_pad=2, pad=0)\n",
    "\n",
    "        self.index = 0\n",
    "        self.fig = fig\n",
    "        self.axs = axs\n",
    "\n",
    "    def finish(self):\n",
    "        plt.show()\n",
    "\n",
    "        self.fig = None\n",
    "        self.axs = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        subplot_i = i // (self.rows - 1)\n",
    "        subplot_j = i % self.cols\n",
    "        subplot = (subplot_i, subplot_j)\n",
    "\n",
    "        return self.axs[subplot]\n",
    "\n",
    "    def __next__(self):\n",
    "        subplot_i = self.index // (self.rows - 1)\n",
    "        subplot_j = self.index % self.cols\n",
    "        subplot = (subplot_i, subplot_j)\n",
    "\n",
    "        self.index += 1\n",
    "\n",
    "        return self.axs[subplot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataframe, labels_dataframe, variable_indexes = (\n",
    "    dataset_manager.process_dataframe_one_hot(dataset_dataframe)\n",
    ")\n",
    "k_fold_cross_validation = KFoldCrossValidation(\n",
    "    features_dataframe, labels_dataframe, variable_indexes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierRunner:\n",
    "    _k_fold_cross_validation: KFoldCrossValidation\n",
    "    _plot_manager: PlotManager\n",
    "    _classifier_generator: t.Callable[[t.Dict], BaseEstimator]\n",
    "    _name: str\n",
    "    _output_label: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        k_fold_cross_validation: KFoldCrossValidation,\n",
    "        plot_manager: PlotManager,\n",
    "        classifier_generator: t.Callable[[t.Dict], BaseEstimator],\n",
    "        name: str,\n",
    "        output_label: str,\n",
    "    ):\n",
    "        self._k_fold_cross_validation = k_fold_cross_validation\n",
    "        self._plot_manager = plot_manager\n",
    "        self._classifier_generator = classifier_generator\n",
    "        self._name = name\n",
    "        self._output_label = output_label\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        params_sets: t.List[t.Dict],\n",
    "        label_combinations: t.List[t.Tuple[str, ...]],\n",
    "        all_features: t.List[str],\n",
    "    ):\n",
    "        best_classifier = None\n",
    "        best_metrics = pd.DataFrame(\n",
    "            {\"Precision score\": [0], \"Recall score\": [0], \"F1 score\": [0]}\n",
    "        )\n",
    "\n",
    "        for params_set in params_sets:\n",
    "            best_classifier, best_metrics = self.run_for_parameter_set(\n",
    "                params_set=params_set,\n",
    "                label_combinations=label_combinations,\n",
    "                all_features=all_features,\n",
    "            )\n",
    "\n",
    "        return best_classifier, best_metrics\n",
    "\n",
    "    def run_for_parameter_set(\n",
    "        self,\n",
    "        params_set: t.Dict,\n",
    "        label_combinations: t.List[t.Tuple[str, ...]],\n",
    "        all_features: t.List[str],\n",
    "    ):\n",
    "        best_classifier = None\n",
    "        best_metrics = pd.DataFrame(\n",
    "            {\"Precision score\": [0], \"Recall score\": [0], \"F1 score\": [0]}\n",
    "        )\n",
    "\n",
    "        self._plot_manager.init()\n",
    "        self._display_header(params_set)\n",
    "\n",
    "        for subplot, label_combination in zip(self._plot_manager, label_combinations):\n",
    "            X, Y = self._k_fold_cross_validation.get_train_val_full(label_combination)\n",
    "\n",
    "            current_classifier, current_metrics, _ = (\n",
    "                self.run_for_k_fold_cross_validation(\n",
    "                    params_set=params_set,\n",
    "                    selected_features=label_combination,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if current_metrics[\"F1 score\"][0] > best_metrics[\"F1 score\"][0]:\n",
    "                best_metrics = current_metrics\n",
    "                best_classifier = current_classifier\n",
    "\n",
    "            DecisionBoundaryDisplay.from_estimator(\n",
    "                OneHotToNumericPipeline(current_classifier),\n",
    "                X,\n",
    "                cmap=plt.cm.RdYlBu,\n",
    "                response_method=\"predict\",\n",
    "                ax=subplot,\n",
    "                xlabel=label_combination[0],\n",
    "                ylabel=label_combination[1],\n",
    "            )\n",
    "\n",
    "            subplot.scatter(\n",
    "                X[:, 0],\n",
    "                X[:, 1],\n",
    "                c=np.argmax(Y, axis=1),\n",
    "                cmap=plt.cm.RdYlBu,\n",
    "                linewidths=0.25,\n",
    "                edgecolor=\"black\",\n",
    "                s=5,\n",
    "            )\n",
    "\n",
    "            self.display_metrics_section(\n",
    "                label_combination=label_combination,\n",
    "                current_metrics=current_metrics,\n",
    "                output_label=self._output_label,\n",
    "            )\n",
    "\n",
    "        self._plot_manager.finish()\n",
    "        self.evaluate_classifier_using_k_fold_for_all(\n",
    "            params_set=params_set,\n",
    "            all_features=all_features,\n",
    "        )\n",
    "\n",
    "        return best_classifier, best_metrics\n",
    "\n",
    "    def _display_header(self, params_set):\n",
    "        display(HTML(f\"<h1>{self._name} classifier</h1>\"))\n",
    "        display(HTML(f\"<p>Parameters: {params_set}.</p>\"))\n",
    "\n",
    "    def display_metrics_section(\n",
    "        self,\n",
    "        label_combination: t.List[t.List[str]],\n",
    "        current_metrics: pd.DataFrame,\n",
    "        output_label: str,\n",
    "    ):\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<p>For input label combination {label_combination} and output label '{output_label}', metrics were:</p>\"\n",
    "            )\n",
    "        )\n",
    "        display(current_metrics)\n",
    "\n",
    "    def run_for_k_fold_cross_validation(\n",
    "        self,\n",
    "        params_set: t.Dict,\n",
    "        selected_features: t.Tuple[str, ...],\n",
    "    ):\n",
    "        current_confusion_matrix = ConfusionMatrix(selected_features)\n",
    "\n",
    "        for it in range(5):\n",
    "            (X_train, Y_train), (X_test, Y_test) = (\n",
    "                k_fold_cross_validation.get_train_val_split(it, selected_features)\n",
    "            )\n",
    "\n",
    "            classifier = self._classifier_generator(params_set)\n",
    "            classifier.fit(X_train, Y_train)\n",
    "\n",
    "            Y_hat = classifier.predict(X_test)\n",
    "            current_confusion_matrix.register_predictions(\n",
    "                arg_y_true=Y_test, arg_y_hat=Y_hat\n",
    "            )\n",
    "\n",
    "        X_train, Y_train = k_fold_cross_validation.get_train_val_full(selected_features)\n",
    "        classifier = self._classifier_generator(params_set)\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        return (\n",
    "            classifier,\n",
    "            current_confusion_matrix.compute_metrics(),\n",
    "            current_confusion_matrix,\n",
    "        )\n",
    "\n",
    "    def evaluate_classifier_using_k_fold_for_all(\n",
    "        self,\n",
    "        params_set: t.Dict,\n",
    "        all_features: t.List[str],\n",
    "    ):\n",
    "        display(HTML(f\"<h1>Classifier with features: {all_features}</h1>\"))\n",
    "\n",
    "        current_classifier, current_metrics, confusion_matrix = (\n",
    "            self.run_for_k_fold_cross_validation(\n",
    "                params_set=params_set,\n",
    "                selected_features=all_features,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display(HTML(f\"<h3>Precision:</h3>\"))\n",
    "        display(\n",
    "            HTML(\n",
    "                pd.DataFrame(\n",
    "                    confusion_matrix.compute_precision_foreach(),\n",
    "                    index=all_output_classes,\n",
    "                ).to_html()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display(HTML(f\"<h3>Recall</h3>\"))\n",
    "        display(\n",
    "            HTML(\n",
    "                pd.DataFrame(\n",
    "                    confusion_matrix.compute_recall_foreach(), index=all_output_classes\n",
    "                ).to_html()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display(HTML(f\"<h3>F1 Score</h3>\"))\n",
    "        display(\n",
    "            HTML(\n",
    "                pd.DataFrame(\n",
    "                    confusion_matrix.compute_f1_foreach(), index=all_output_classes\n",
    "                ).to_html()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        confusion_matrix.plot_confusion_matrix()\n",
    "\n",
    "\n",
    "class ClassifierRunnerWithPresetFeatures:\n",
    "    _label_combinations: t.List[t.Tuple[str, ...]]\n",
    "    _all_features: t.List[str]\n",
    "    _classifier_runner: ClassifierRunner\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_combinations: t.List[t.Tuple[str, ...]],\n",
    "        all_features: t.List[str],\n",
    "        classifier_runner: ClassifierRunner,\n",
    "    ) -> None:\n",
    "        self._label_combinations = label_combinations\n",
    "        self._all_features = all_features\n",
    "        self._classifier_runner = classifier_runner\n",
    "\n",
    "    def run(self, params_set: t.Dict):\n",
    "        return self._classifier_runner.run_for_parameter_set(\n",
    "            all_features=self._all_features,\n",
    "            label_combinations=self._label_combinations,\n",
    "            params_set=params_set,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyMetaOptimiser:\n",
    "    _classifier_runner: ClassifierRunnerWithPresetFeatures\n",
    "    _k: int\n",
    "    _patience: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier_runner: ClassifierRunnerWithPresetFeatures,\n",
    "        k: int = 10,\n",
    "        patience: int = 5,\n",
    "    ) -> None:\n",
    "        self._classifier_runner = classifier_runner\n",
    "        self._k = k\n",
    "        self._patience = patience\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        params_set: t.Dict[str, t.Tuple[t.Union[float, str]]],\n",
    "        params_set_variants: t.Dict[str, t.Tuple[t.Union[float, str]]],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Greedy, hillclimber-like hyperparameter meta-optimiser, with patience functionality.\n",
    "\n",
    "        For numerical, params_set_variants[key] is a tuple of steps. E.g.: (1, -1, 0.1, -0.1)\n",
    "        For categorial, params_set_variants[key] is a tuple of categories. E.g.: (\"gini\", \"log-entropy\")\n",
    "        \"\"\"\n",
    "\n",
    "        best_params = params_set\n",
    "        best_classifier = None\n",
    "        best_metrics = pd.DataFrame(\n",
    "            {\"Precision score\": [0], \"Recall score\": [0], \"F1 score\": [0]}\n",
    "        )\n",
    "        previous_best_metrics = best_metrics\n",
    "        current_patience = self._patience\n",
    "\n",
    "        for i in range(self._k):\n",
    "            params_set_trials = self.build_params_trials(\n",
    "                params_set=best_params, params_set_variants=params_set_variants\n",
    "            )\n",
    "            (\n",
    "                best_classifier,\n",
    "                best_metrics,\n",
    "                best_params,\n",
    "            ) = self.pick_best_params(\n",
    "                params_set_trials=params_set_trials,\n",
    "                best_classifier=best_classifier,\n",
    "                best_metrics=best_metrics,\n",
    "                best_params=best_params,\n",
    "            )\n",
    "\n",
    "            if current_patience <= 0:\n",
    "                break\n",
    "\n",
    "            if previous_best_metrics[\"F1 score\"][0] < best_metrics[\"F1 score\"][0]:\n",
    "                current_patience = self._patience\n",
    "            else:\n",
    "                current_patience -= 1\n",
    "\n",
    "            previous_best_metrics = best_metrics\n",
    "\n",
    "            self.display_state(best_params, best_metrics, i)\n",
    "\n",
    "        return best_params, best_classifier, best_metrics\n",
    "\n",
    "    def display_state(\n",
    "        self, best_params: pd.DataFrame, best_metrics: pd.DataFrame, i: int\n",
    "    ):\n",
    "        print(\n",
    "            f\"Iteration {i}: best F1 score = {best_metrics['F1 score'][0]}, best_params = {best_params}\"\n",
    "        )\n",
    "\n",
    "    def build_params_trials(self, params_set: t.Dict, params_set_variants: t.Dict):\n",
    "        params_set_trials = []\n",
    "        params_set_copy = copy.copy(params_set)\n",
    "\n",
    "        for params_set_key in params_set_copy.keys():\n",
    "            param = params_set_copy.get(params_set_key, None)\n",
    "            param_variants = params_set_variants.get(params_set_key)\n",
    "\n",
    "            if not param_variants:\n",
    "                continue\n",
    "\n",
    "            picked_param_variant = np.random.randint(0, len(param_variants))\n",
    "\n",
    "            if type(param) == int:\n",
    "                params_set_copy[params_set_key] = int(\n",
    "                    params_set_copy[params_set_key]\n",
    "                    * param_variants[picked_param_variant]\n",
    "                )\n",
    "\n",
    "            elif type(param) == float:\n",
    "                params_set_copy[params_set_key] *= param_variants[picked_param_variant]\n",
    "\n",
    "            elif type(param) == str:\n",
    "                params_set_copy[params_set_key] = param_variants[picked_param_variant]\n",
    "\n",
    "            params_set_trials.append(params_set_copy)\n",
    "\n",
    "        return params_set_trials\n",
    "\n",
    "    def pick_best_params(\n",
    "        self,\n",
    "        params_set_trials: t.List[t.Dict],\n",
    "        best_classifier: BaseEstimator,\n",
    "        best_metrics: pd.DataFrame,\n",
    "        best_params: t.Dict,\n",
    "    ):\n",
    "        for params_set_trial in params_set_trials:\n",
    "            classifier, metrics = self._classifier_runner.run(\n",
    "                params_set=params_set_trial\n",
    "            )\n",
    "\n",
    "            if metrics[\"F1 score\"][0] > best_metrics[\"F1 score\"][0]:\n",
    "                best_classifier = classifier\n",
    "                best_metrics = metrics\n",
    "                best_params = params_set_trial\n",
    "\n",
    "        return (\n",
    "            best_classifier,\n",
    "            best_metrics,\n",
    "            best_params,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"best\"},\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"random\"},\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: DecisionTreeClassifier(\n",
    "        criterion=params_set[\"criterion\"], splitter=params_set[\"splitter\"]\n",
    "    ),\n",
    "    name=\"Decision trees\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decision tree with greedy hyperparameter meta-optimisation\n",
    "\"\"\"\n",
    "\n",
    "params_set = {\"criterion\": \"gini\", \"splitter\": \"best\"}\n",
    "params_set_variants = {\n",
    "    \"criterion\": (\"gini\", \"entropy\", \"log_loss\"),\n",
    "    \"splitter\": (\"best\", \"random\"),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: DecisionTreeClassifier(\n",
    "            criterion=params_set[\"criterion\"], splitter=params_set[\"splitter\"]\n",
    "        ),\n",
    "        name=\"Decision trees\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RandomForest or ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"best\"},\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"random\"},\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: ExtraTreeClassifier(\n",
    "        criterion=params_set[\"criterion\"], splitter=params_set[\"splitter\"]\n",
    "    ),\n",
    "    name=\"Decision trees\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ExtraTrees with greedy hyperparameter meta-optimisation\n",
    "\"\"\"\n",
    "\n",
    "params_set = {\"criterion\": \"gini\", \"splitter\": \"best\"}\n",
    "params_set_variants = {\n",
    "    \"criterion\": (\"gini\", \"entropy\", \"log_loss\"),\n",
    "    \"splitter\": (\"best\", \"random\"),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: ExtraTreeClassifier(\n",
    "            criterion=params_set[\"criterion\"], splitter=params_set[\"splitter\"]\n",
    "        ),\n",
    "        name=\"ExtraTrees\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\n",
    "        \"max_depth\": 2,\n",
    "        \"learning_rate\": 0.3,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": dataset_output_classes,\n",
    "        \"rounds\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"max_depth\": 2,\n",
    "        \"learning_rate\": 0.2,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": dataset_output_classes,\n",
    "        \"rounds\": 200,\n",
    "    },\n",
    "    {\n",
    "        \"max_depth\": 2,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": dataset_output_classes,\n",
    "        \"rounds\": 500,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "        xgb.XGBClassifier(\n",
    "            max_depth=params_set[\"max_depth\"],\n",
    "            learning_rate=params_set[\"learning_rate\"],\n",
    "            n_estimators=params_set[\"rounds\"],\n",
    "            objective=params_set[\"objective\"],\n",
    "            tree_method=\"hist\",\n",
    "        )\n",
    "    ),\n",
    "    name=\"XGBoost\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBoost with greedy hyperparameter meta-optimisation\n",
    "\"\"\"\n",
    "\n",
    "params_set = {\n",
    "    \"max_depth\": 2,\n",
    "    \"learning_rate\": 0.3,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": dataset_output_classes,\n",
    "    \"rounds\": 100,\n",
    "}\n",
    "params_set_variants = {\n",
    "    \"rounds\": (2, 1, 0.5),\n",
    "    \"learning_rate\": (2, 1, 0.1),\n",
    "    \"rounds\": (1, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "            xgb.XGBClassifier(\n",
    "                max_depth=params_set[\"max_depth\"],\n",
    "                learning_rate=params_set[\"learning_rate\"],\n",
    "                n_estimators=params_set[\"rounds\"],\n",
    "                objective=params_set[\"objective\"],\n",
    "                tree_method=\"hist\",\n",
    "            )\n",
    "        ),\n",
    "        name=\"XGBoost trees\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\"alpha\": 0.25, \"force_alpha\": True},\n",
    "    {\"alpha\": 0.50, \"force_alpha\": True},\n",
    "    {\"alpha\": 1.00, \"force_alpha\": True},\n",
    "    {\"alpha\": 2.00, \"force_alpha\": True},\n",
    "    {\"alpha\": 2.50, \"force_alpha\": True},\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "        MultinomialNB(\n",
    "            alpha=params_set[\"alpha\"],\n",
    "            force_alpha=params_set[\"force_alpha\"],\n",
    "        )\n",
    "    ),\n",
    "    name=\"Naive Bayes\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Naive Bayes with greedy hyperparameter meta-optimisation\n",
    "\"\"\"\n",
    "\n",
    "params_set = {\"alpha\": 0.25, \"force_alpha\": True}\n",
    "params_set_variants = {\n",
    "    \"alpha\": (1.5, 1.25, 1, 0.75, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "            MultinomialNB(\n",
    "                alpha=params_set[\"alpha\"],\n",
    "                force_alpha=params_set[\"force_alpha\"],\n",
    "            )\n",
    "        ),\n",
    "        name=\"Naive Bayes\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"ball_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"ball_tree\", \"metric\": \"cityblock\"},\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"kd_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"kd_tree\", \"metric\": \"cityblock\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"ball_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"ball_tree\", \"metric\": \"cityblock\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"kd_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"kd_tree\", \"metric\": \"cityblock\"},\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: KNeighborsClassifier(\n",
    "        n_neighbors=params_set[\"n_neighbors\"],\n",
    "        algorithm=params_set[\"algorithm\"],\n",
    "        metric=params_set[\"metric\"],\n",
    "    ),\n",
    "    name=\"K-NN\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")\n",
    "\n",
    "classifier_runner.evaluate_classifier_using_k_fold_for_all(\n",
    "    params_set={\"n_neighbors\": 5, \"algorithm\": \"ball_tree\", \"metric\": \"minkowski\"},\n",
    "    all_features=ALL_VARIABLES_NO_LABEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_set = {\n",
    "    \"n_neighbors\": 5,\n",
    "    \"algorithm\": \"ball_tree\",\n",
    "    \"metric\": \"minkowski\",\n",
    "}\n",
    "params_set_variants = {\n",
    "    \"n_neighbors\": (1.25, 0.75),\n",
    "    \"algorithm\": (\"ball_tree\", \"kd_tree\"),\n",
    "    \"metric\": (\"minkowski\", \"cityblock\"),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: KNeighborsClassifier(\n",
    "            n_neighbors=params_set[\"n_neighbors\"],\n",
    "            algorithm=params_set[\"algorithm\"],\n",
    "            metric=params_set[\"metric\"],\n",
    "        ),\n",
    "        name=\"K-NN\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNModel(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=10,\n",
    "        epochs=4,\n",
    "        lr=0.001,\n",
    "        hidden_activation=\"relu\",\n",
    "        optimizer=\"adam\",\n",
    "    ):\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.model = None\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def make_model(self, size: int):\n",
    "        assert self.model is None\n",
    "\n",
    "        self.model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(size,)),\n",
    "                layers.Dense(128, activation=self.hidden_activation),\n",
    "                layers.Dense(64, activation=self.hidden_activation),\n",
    "                layers.Dense(7, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.make_model(np.shapeJ(X)[1])\n",
    "        self.scaler.fit(X)\n",
    "\n",
    "        return self.model.fit(\n",
    "            self.scaler.transform(X), y, batch_size=self.batch_size, epochs=self.epochs\n",
    "        )\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return (self.model.predict(self.scaler.transform(X)) > 0.5).astype(bool)\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return [np.array([False, True]) for _ in range(7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\"lr\": 0.004, \"epochs\": 10, \"hidden_activation\": \"relu\", \"optimizer\": \"adam\"},\n",
    "    {\"lr\": 0.01, \"epochs\": 10, \"hidden_activation\": \"relu\", \"optimizer\": \"adam\"},\n",
    "    {\"lr\": 0.01, \"epochs\": 10, \"hidden_activation\": \"relu\", \"optimizer\": \"sgd\"},\n",
    "    {\"lr\": 0.002, \"epochs\": 5, \"hidden_activation\": \"relu\", \"optimizer\": \"adam\"},\n",
    "    {\"lr\": 0.002, \"epochs\": 10, \"hidden_activation\": \"sigmoid\", \"optimizer\": \"adam\"},\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: RNModel(\n",
    "        lr=params_set[\"lr\"], epochs=params_set[\"epochs\"]\n",
    "    ),\n",
    "    name=\"Neural Network\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural networks with greedy hyperparameter meta-optimisation\n",
    "\"\"\"\n",
    "\n",
    "params_set = {\"lr\": 0.002, \"epochs\": 5, \"hidden_activation\": \"relu\", \"optimizer\": \"adam\"}\n",
    "params_set_variants = {\n",
    "    \"lr\": (2, 1.5, 1, 0.75, 0.5),\n",
    "    \"epochs\": (2, 1.5, 1, 0.5, 0.25),\n",
    "    \"hidden_activation\": (\"relu\", \"sigmoid\"),\n",
    "    \"optimizer\": (\"adam\", \"sgd\"),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: RNModel(\n",
    "            lr=params_set[\"lr\"], epochs=params_set[\"epochs\"]\n",
    "        ),\n",
    "        name=\"Neural Network\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\"kernel\": \"rbf\", \"gamma\": \"auto\"},\n",
    "    {\"kernel\": \"sigmoid\", \"gamma\": \"auto\"},\n",
    "    {\"kernel\": \"poly\", \"gamma\": \"auto\"},\n",
    "    {\"kernel\": \"linear\", \"gamma\": \"auto\"},\n",
    "    {\"kernel\": \"rbf\", \"gamma\": 0.1},\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SVC(kernel=params_set[\"kernel\"], gamma=params_set[\"gamma\"]),\n",
    "        )\n",
    "    ),\n",
    "    name=\"SVM\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")\n",
    "\n",
    "classifier_runner.evaluate_classifier_using_k_fold_for_all(\n",
    "    params_set={\"kernel\": \"rbf\", \"gamma\": \"auto\"},\n",
    "    all_features=ALL_VARIABLES_NO_LABEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVM with greedy hyperparameter meta-optimisation\n",
    "\"\"\"\n",
    "\n",
    "params_set = {\"kernel\": \"rbf\", \"gamma\": \"auto\", \"C\": 1.0, \"degree\": 3}\n",
    "params_set_variants = {\n",
    "    \"kernel\": (\"rbf\", \"sigmoid\", \"poly\", \"linear\"),\n",
    "    \"C\": (2, 1.5, 1, 0.75, 0.5),\n",
    "    \"degree\": (2, 1.5, 1, 0.75, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "            make_pipeline(\n",
    "                StandardScaler(),\n",
    "                SVC(kernel=params_set[\"kernel\"], gamma=params_set[\"gamma\"]),\n",
    "            )\n",
    "        ),\n",
    "        name=\"SVM\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\"penalty\": \"l2\", \"tol\": 1e-04, \"C\": 1.0, \"intercept_scaling\": 0.25},\n",
    "    {\"penalty\": \"l2\", \"tol\": 1e-04, \"C\": 1.0, \"intercept_scaling\": 0.50},\n",
    "    {\"penalty\": \"l2\", \"tol\": 1e-04, \"C\": 1.0, \"intercept_scaling\": 1.00},\n",
    "    {\"penalty\": \"l2\", \"tol\": 1e-04, \"C\": 0.5, \"intercept_scaling\": 0.25},\n",
    "    {\"penalty\": \"l2\", \"tol\": 1e-04, \"C\": 0.5, \"intercept_scaling\": 0.50},\n",
    "    {\"penalty\": \"l2\", \"tol\": 1e-04, \"C\": 0.5, \"intercept_scaling\": 1.00},\n",
    "]\n",
    "\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "        LogisticRegression(\n",
    "            penalty=params_set[\"penalty\"],\n",
    "            tol=params_set[\"tol\"],\n",
    "            C=params_set[\"C\"],\n",
    "            intercept_scaling=params_set[\"intercept_scaling\"],\n",
    "        )\n",
    "    ),\n",
    "    name=\"Logistic Regression\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Naive Bayes with greedy hyperparameter meta-optimisation\n",
    "\"\"\"\n",
    "\n",
    "params_set = {\"penalty\": \"l2\", \"tol\": 1e-04, \"C\": 1.0, \"intercept_scaling\": 0.25}\n",
    "params_set_variants = {\n",
    "    \"C\": (1.5, 1.25, 1, 0.75, 0.5),\n",
    "    \"intercept_scaling\": (1.5, 1.25, 1, 0.75, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "classifier_runner_with_preset_features = ClassifierRunnerWithPresetFeatures(\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    "    classifier_runner=ClassifierRunner(\n",
    "        k_fold_cross_validation=k_fold_cross_validation,\n",
    "        plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "        classifier_generator=lambda params_set: NumericOnlyPipeline(\n",
    "            LogisticRegression(\n",
    "                penalty=params_set[\"penalty\"],\n",
    "                tol=params_set[\"tol\"],\n",
    "                C=params_set[\"C\"],\n",
    "                intercept_scaling=params_set[\"intercept_scaling\"],\n",
    "            )\n",
    "        ),\n",
    "        name=\"Logistic Regression\",\n",
    "        output_label=LABEL_VARIABLE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "greedy_meta_optimiser = GreedyMetaOptimiser(\n",
    "    classifier_runner=classifier_runner_with_preset_features\n",
    ")\n",
    "\n",
    "_ = greedy_meta_optimiser.run(\n",
    "    params_set=params_set,\n",
    "    params_set_variants=params_set_variants,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedModel(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, lh_model_config: dict, rh_model_config: dict, logistic: dict):\n",
    "        self.lh_model = RNModel(**lh_model_config)\n",
    "        self.rh_model = ExtraTreeClassifier(**rh_model_config)\n",
    "\n",
    "        self.logistic_regression = NumericOnlyPipeline(\n",
    "            LogisticRegression(\n",
    "                penalty=logistic[\"penalty\"],\n",
    "                tol=logistic[\"tol\"],\n",
    "                C=logistic[\"C\"],\n",
    "                intercept_scaling=logistic[\"intercept_scaling\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.lh_model.fit(X, y)\n",
    "        self.rh_model.fit(X, y)\n",
    "        return self.logistic_regression.fit(\n",
    "            np.hstack((self.lh_model.predict(X), self.rh_model.predict(X))), y\n",
    "        )\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.logistic_regression.predict(\n",
    "            np.hstack((self.lh_model.predict(X), self.rh_model.predict(X)))\n",
    "        )\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return [np.array([False, True]) for _ in range(7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_sets = [\n",
    "    {\n",
    "        \"lh_model_config\": {\"lr\": 0.002, \"epochs\": 1},\n",
    "        \"rh_model_config\": {\"criterion\": \"gini\", \"splitter\": \"best\"},\n",
    "        \"logistic\": {\n",
    "            \"penalty\": \"l2\",\n",
    "            \"tol\": 1e-04,\n",
    "            \"C\": 1.0,\n",
    "            \"intercept_scaling\": 0.25,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "classifier_runner = ClassifierRunner(\n",
    "    k_fold_cross_validation=k_fold_cross_validation,\n",
    "    plot_manager=PlotManager(cols=3, samples=len(SELECTED_FEATURES_COMBINATIONS)),\n",
    "    classifier_generator=lambda params_set: StackedModel(\n",
    "        lh_model_config=params_set[\"lh_model_config\"],\n",
    "        rh_model_config=params_set[\"rh_model_config\"],\n",
    "        logistic=params_set[\"logistic\"],\n",
    "    ),\n",
    "    name=\"Stacked model (Neural network + extra trees + logistic regression)\",\n",
    "    output_label=LABEL_VARIABLE,\n",
    ")\n",
    "_ = classifier_runner.run(\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=SELECTED_FEATURES_COMBINATIONS,\n",
    "    all_features=SELECTED_FEATURES,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
