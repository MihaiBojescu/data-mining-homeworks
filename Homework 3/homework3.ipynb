{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - classifiers\n",
    "\n",
    "In this homework we will use 7 algorithms to classify data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite knowledge\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Precision &= \\frac{\\text{True positives}}{\\text{True positives} + \\text{False positives}} \\\\\n",
    "    Recall &= \\frac{\\text{True positives}}{\\text{True positives} + \\text{False negatives}} \\\\\n",
    "    F1 &= 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Note**: This is mostly for learning purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    os.environ[\"R_HOME\"] = \"C:\\Program Files\\R\\R-4.3.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset-specific dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_VARIABLE = \"NObeyesdad\"\n",
    "NUMERICAL_VARIABLES = [\"Age\", \"Height\", \"Weight\", \"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
    "CATEGORICAL_VARIABLES_NO_LABEL = [\n",
    "    \"FAVC\",\n",
    "    \"CAEC\",\n",
    "    \"CALC\",\n",
    "    \"SCC\",\n",
    "    \"MTRANS\",\n",
    "    \"Gender\",\n",
    "    \"family_history_with_overweight\",\n",
    "    \"SMOKE\",\n",
    "]\n",
    "CATEGORICAL_VARIABLES = [\n",
    "    *CATEGORICAL_VARIABLES_NO_LABEL,\n",
    "    LABEL_VARIABLE,\n",
    "]\n",
    "ALL_VARIABLES_NO_LABEL = [*NUMERICAL_VARIABLES, *CATEGORICAL_VARIABLES_NO_LABEL]\n",
    "ALL_VARIABLES = [*NUMERICAL_VARIABLES, *CATEGORICAL_VARIABLES]\n",
    "LABEL_DICTIONARY = {\n",
    "    \"Age\": \"Age\",\n",
    "    \"Height\": \"Height (cm)\",\n",
    "    \"Weight\": \"Weight (kg)\",\n",
    "    \"FCVC\": \" Frequency of consumption of vegetables (times per day)\",\n",
    "    \"NCP\": \"Number of main meals\",\n",
    "    \"CH2O\": \"Consumption of water daily (Liters)\",\n",
    "    \"FAF\": \"Physical activity frequency (times per day)\",\n",
    "    \"TUE\": \"Time using technology devices (hours)\",\n",
    "    \"FAVC\": \"Frequent consumption of high caloric food\",\n",
    "    \"CAEC\": \"Consumption of food between meals\",\n",
    "    \"CALC\": \"Consumption of alcohol\",\n",
    "    \"SCC\": \"Calories consumption monitoring\",\n",
    "    \"MTRANS\": \"Transportation used\",\n",
    "    \"Gender\": \"Gender\",\n",
    "    \"family_history_with_overweight\": \"Family member suffered or suffers from overweight\",\n",
    "    \"SMOKE\": \"Smoker or not\",\n",
    "    \"NObeyesdad\": \"Obesity level\",\n",
    "}\n",
    "\n",
    "T = t.TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Person:\n",
    "    Gender: str\n",
    "    Age: np.int32\n",
    "    Height: np.float32\n",
    "    Weight: np.float32\n",
    "    family_history_with_overweight: str\n",
    "    FAVC: str\n",
    "    FCVC: np.float32\n",
    "    NCP: np.float32\n",
    "    CAEC: str\n",
    "    SMOKE: str\n",
    "    CH2O: np.float32\n",
    "    SCC: str\n",
    "    FAF: np.float32\n",
    "    TUE: np.float32\n",
    "    CALC: str\n",
    "    MTRANS: str\n",
    "    NObeyesdad: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        Gender: str,\n",
    "        Age: str,\n",
    "        Height: str,\n",
    "        Weight: str,\n",
    "        family_history_with_overweight: str,\n",
    "        FAVC: str,\n",
    "        FCVC: str,\n",
    "        NCP: str,\n",
    "        CAEC: str,\n",
    "        SMOKE: str,\n",
    "        CH2O: str,\n",
    "        SCC: str,\n",
    "        FAF: str,\n",
    "        TUE: str,\n",
    "        CALC: str,\n",
    "        MTRANS: str,\n",
    "        NObeyesdad: str,\n",
    "    ):\n",
    "        self.Gender = Gender\n",
    "        self.Age = np.float32(Age)\n",
    "        self.Height = np.float32(Height)\n",
    "        self.Weight = np.float32(Weight)\n",
    "        self.family_history_with_overweight = family_history_with_overweight\n",
    "        self.FAVC = FAVC\n",
    "        self.FCVC = np.float32(FCVC)\n",
    "        self.NCP = np.float32(NCP)\n",
    "        self.CAEC = CAEC\n",
    "        self.SMOKE = SMOKE\n",
    "        self.CH2O = np.float32(CH2O)\n",
    "        self.SCC = SCC\n",
    "        self.FAF = np.float32(FAF)\n",
    "        self.TUE = np.float32(TUE)\n",
    "        self.CALC = CALC\n",
    "        self.MTRANS = MTRANS\n",
    "        self.NObeyesdad = NObeyesdad\n",
    "\n",
    "    def __str__(self):\n",
    "        return vars(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(vars(self))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return vars(self)\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self, path_to_csv: str):\n",
    "        self.path_to_csv = path_to_csv\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def load_as_obj_list(self) -> list[Person]:\n",
    "        with open(self.path_to_csv) as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            return [Person(**row) for row in csv_reader]\n",
    "\n",
    "    def make_label_combinations(self, labels: t.List[str], k: int = 2):\n",
    "        result: t.List[t.Tuple[str, ...]] = []\n",
    "\n",
    "        for combination in itertools.combinations(labels, k):\n",
    "            result.append(list(combination))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def labels_to_onehot(\n",
    "        self, dataframe: pd.DataFrame, labels: t.List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        return pd.get_dummies(dataframe[labels], drop_first=True, dtype=int)\n",
    "\n",
    "    def sample(\n",
    "        self, dataframe: pd.DataFrame, frac: float, random_state: t.Optional[int] = 42\n",
    "    ):\n",
    "        sample = dataframe.sample(frac=frac, random_state=random_state)\n",
    "        sample_extra = dataframe.drop(sample.index)\n",
    "\n",
    "        return (sample, sample_extra)\n",
    "\n",
    "    def encode_categorial_to_numerical(self, dataframe: pd.DataFrame):\n",
    "        return pd.Series(data=self.label_encoder.fit_transform(dataframe))\n",
    "\n",
    "    def compute_metrics(self, y_true: pd.DataFrame, y_hat: pd.DataFrame):\n",
    "        precision = precision_score(y_true, y_hat, average=\"micro\")\n",
    "        recall = recall_score(y_true, y_hat, average=\"micro\")\n",
    "        f1 = f1_score(y_true, y_hat, average=\"micro\")\n",
    "\n",
    "        result = pd.DataFrame(\n",
    "            {\n",
    "                \"Precision score\": [precision],\n",
    "                \"Recall score\": [recall],\n",
    "                \"F1 score\": [f1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotManager:\n",
    "    def __init__(self, cols: int, samples: int) -> None:\n",
    "        self.rows = samples // cols + 1\n",
    "        self.cols = cols\n",
    "        self.samples = samples\n",
    "        self.index = 0\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig, axs = plt.subplots(self.rows, self.cols)\n",
    "        plt.tight_layout(h_pad=2, w_pad=2, pad=0)\n",
    "\n",
    "        self.fig = fig\n",
    "        self.axs = axs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        subplot_i = i // (self.rows - 1)\n",
    "        subplot_j = i % self.cols\n",
    "        subplot = (subplot_i, subplot_j)\n",
    "\n",
    "        return self.axs[subplot]\n",
    "\n",
    "    def __next__(self):\n",
    "        subplot_i = self.index // (self.rows - 1)\n",
    "        subplot_j = self.index % self.cols\n",
    "        subplot = (subplot_i, subplot_j)\n",
    "\n",
    "        self.index += 1\n",
    "\n",
    "        return self.axs[subplot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(\"data/ObesityDataSet.csv\")\n",
    "dataset_obj_list = dataset_manager.load_as_obj_list()\n",
    "dataset_dataframe = pd.DataFrame.from_records(\n",
    "    data=[vars(entry) for entry in dataset_obj_list]\n",
    ")\n",
    "dataset_output_classes = len(dataset_dataframe[LABEL_VARIABLE].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Properly implement this\n",
    "def generate_k_folds(X: pd.DataFrame, Y: pd.DataFrame, k: int = 8):\n",
    "    random_state = 42\n",
    "\n",
    "    X_train, X_test = dataset_manager.sample(\n",
    "        dataframe=X, frac=0.8, random_state=random_state\n",
    "    )\n",
    "    Y_train, Y_test = dataset_manager.sample(\n",
    "        dataframe=Y, frac=0.8, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return [(X_train, X_test, Y_train, Y_test)]\n",
    "\n",
    "\n",
    "def run_for_k_fold_cross_validation(\n",
    "    dataset_manager: DatasetManager,\n",
    "    classifier_generator: t.Callable[[t.Dict], BaseEstimator],\n",
    "    params_set: t.Dict,\n",
    "    X: pd.DataFrame,\n",
    "    Y: pd.DataFrame,\n",
    "):\n",
    "    best_current_classifier = None\n",
    "    best_current_metrics = pd.DataFrame(\n",
    "        {\"Precision score\": [0], \"Recall score\": [0], \"F1 score\": [0]}\n",
    "    )\n",
    "\n",
    "    for X_train, X_test, Y_train, Y_test in generate_k_folds(X=X, Y=Y):\n",
    "        classifier = classifier_generator(params_set)\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        Y_hat = classifier.predict(X_test)\n",
    "        Y_hat = pd.DataFrame({LABEL_VARIABLE: Y_hat})\n",
    "\n",
    "        metrics = dataset_manager.compute_metrics(y_true=Y_test, y_hat=Y_hat)\n",
    "\n",
    "        if metrics[\"F1 score\"][0] > best_current_metrics[\"F1 score\"][0]:\n",
    "            best_current_metrics = metrics\n",
    "            best_current_classifier = classifier\n",
    "\n",
    "    return best_current_classifier, best_current_metrics\n",
    "\n",
    "\n",
    "def run_for_parameter_set(\n",
    "    dataset_manager: DatasetManager,\n",
    "    dataset_dataframe: pd.DataFrame,\n",
    "    classifier_generator: t.Callable[[t.Dict], BaseEstimator],\n",
    "    params_set: t.Dict,\n",
    "    label_combinations: t.List[t.Tuple[str, ...]],\n",
    "    name: str,\n",
    "    random_state: int,\n",
    "    run_for_k_fold_cross_validation_fn: t.Callable[\n",
    "        [\n",
    "            DatasetManager,\n",
    "            t.Callable[[t.Dict], BaseEstimator],\n",
    "            t.List[t.Dict],\n",
    "            pd.DataFrame,\n",
    "            pd.DataFrame,\n",
    "        ],\n",
    "        t.Tuple[BaseEstimator, pd.DataFrame],\n",
    "    ] = run_for_k_fold_cross_validation,\n",
    "):\n",
    "    best_classifier = None\n",
    "    best_metrics = pd.DataFrame(\n",
    "        {\"Precision score\": [0], \"Recall score\": [0], \"F1 score\": [0]}\n",
    "    )\n",
    "\n",
    "    plot_manager = PlotManager(cols=3, samples=len(label_combinations))\n",
    "\n",
    "    display(HTML(f\"<h1>{name} classifier</h1>\"))\n",
    "    display(HTML(f\"<p>Parameters: {params_set}, random state: {random_state}.</p>\"))\n",
    "\n",
    "    for i, label_combination in enumerate(label_combinations):\n",
    "        subplot = plot_manager[i]\n",
    "        best_current_classifier = None\n",
    "        best_current_metrics = pd.DataFrame(\n",
    "            {\"Precision score\": [0], \"Recall score\": [0], \"F1 score\": [0]}\n",
    "        )\n",
    "\n",
    "        X = dataset_manager.labels_to_onehot(dataset_dataframe, label_combination)\n",
    "        Y = dataset_dataframe[LABEL_VARIABLE]\n",
    "        Y_encoded = dataset_manager.encode_categorial_to_numerical(Y)\n",
    "\n",
    "        best_current_classifier, best_current_metrics = (\n",
    "            run_for_k_fold_cross_validation_fn(\n",
    "                dataset_manager=dataset_manager,\n",
    "                classifier_generator=classifier_generator,\n",
    "                params_set=params_set,\n",
    "                X=X,\n",
    "                Y=Y_encoded,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if best_current_metrics[\"F1 score\"][0] > best_metrics[\"F1 score\"][0]:\n",
    "            best_metrics = best_current_metrics\n",
    "            best_classifier = best_current_classifier\n",
    "\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            best_current_classifier,\n",
    "            X,\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            response_method=\"predict\",\n",
    "            ax=subplot,\n",
    "            xlabel=label_combination[0],\n",
    "            ylabel=label_combination[1],\n",
    "        )\n",
    "\n",
    "        subplot.scatter(\n",
    "            X[X.columns[0]],\n",
    "            X[X.columns[1]],\n",
    "            c=Y_encoded,\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            linewidths=0.25,\n",
    "            edgecolor=\"black\",\n",
    "            s=5,\n",
    "        )\n",
    "\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<p>For input label combination {label_combination} and output label '{LABEL_VARIABLE}', best metrics were:</p>\"\n",
    "            )\n",
    "        )\n",
    "        display(best_current_metrics)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return best_classifier, best_metrics\n",
    "\n",
    "\n",
    "def run(\n",
    "    dataset_manager: DatasetManager,\n",
    "    dataset_dataframe: pd.DataFrame,\n",
    "    classifier_generator: t.Callable[[t.Dict], BaseEstimator],\n",
    "    params_sets: t.List[t.Dict],\n",
    "    label_combinations: t.List[t.Tuple[str, ...]],\n",
    "    name: str,\n",
    "    random_state: int,\n",
    "    run_for_parameter_set_fn: t.Callable[\n",
    "        [\n",
    "            DatasetManager,\n",
    "            pd.DataFrame,\n",
    "            t.Callable[[t.Dict], BaseEstimator],\n",
    "            t.List[t.Dict],\n",
    "            t.List[t.Tuple[str, ...]],\n",
    "            str,\n",
    "            int,\n",
    "        ],\n",
    "        t.Tuple[BaseEstimator, pd.DataFrame],\n",
    "    ] = run_for_parameter_set,\n",
    "):\n",
    "    best_classifier = None\n",
    "    best_metrics = pd.DataFrame(\n",
    "        {\"Precision score\": [0], \"Recall score\": [0], \"F1 score\": [0]}\n",
    "    )\n",
    "\n",
    "    for params_set in params_sets:\n",
    "        best_classifier, best_metrics = run_for_parameter_set_fn(\n",
    "            dataset_manager=dataset_manager,\n",
    "            dataset_dataframe=dataset_dataframe,\n",
    "            classifier_generator=classifier_generator,\n",
    "            params_set=params_set,\n",
    "            label_combinations=label_combinations,\n",
    "            name=name,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "    return best_classifier, best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform better selection\n",
    "\n",
    "SELECTED_FEATURES = [\n",
    "    \"NCP\",\n",
    "    \"Height\",\n",
    "    \"Weight\",\n",
    "    \"FAVC\",\n",
    "    \"family_history_with_overweight\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_combinations = dataset_manager.make_label_combinations(SELECTED_FEATURES)\n",
    "random_state = np.random.randint(low=0, high=100)\n",
    "params_sets = [\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"best\"},\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"random\"},\n",
    "]\n",
    "\n",
    "\n",
    "run(\n",
    "    dataset_manager=dataset_manager,\n",
    "    dataset_dataframe=dataset_dataframe,\n",
    "    classifier_generator=lambda params_set: tree.DecisionTreeClassifier(\n",
    "        criterion=params_set[\"criterion\"], splitter=params_set[\"splitter\"]\n",
    "    ),\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=label_combinations,\n",
    "    name=\"Decision trees\",\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RandomForest or ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_combinations = dataset_manager.make_label_combinations(SELECTED_FEATURES)\n",
    "random_state = np.random.randint(low=0, high=100)\n",
    "params_sets = [\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"best\"},\n",
    "    {\"criterion\": \"gini\", \"splitter\": \"random\"},\n",
    "]\n",
    "\n",
    "run(\n",
    "    dataset_manager=dataset_manager,\n",
    "    dataset_dataframe=dataset_dataframe,\n",
    "    classifier_generator=lambda params_set: tree.ExtraTreeClassifier(\n",
    "        criterion=params_set[\"criterion\"], splitter=params_set[\"splitter\"]\n",
    "    ),\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=label_combinations,\n",
    "    name=\"ExtraTree\",\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_combinations = dataset_manager.make_label_combinations(SELECTED_FEATURES)\n",
    "random_state = np.random.randint(low=0, high=100)\n",
    "params_sets = [\n",
    "    {\n",
    "        \"max_depth\": 2,\n",
    "        \"learning_rate\": 0.3,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": dataset_output_classes,\n",
    "        \"rounds\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"max_depth\": 2,\n",
    "        \"learning_rate\": 0.2,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": dataset_output_classes,\n",
    "        \"rounds\": 200,\n",
    "    },\n",
    "    {\n",
    "        \"max_depth\": 2,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": dataset_output_classes,\n",
    "        \"rounds\": 500,\n",
    "    },\n",
    "]\n",
    "\n",
    "run(\n",
    "    dataset_manager=dataset_manager,\n",
    "    dataset_dataframe=dataset_dataframe,\n",
    "    classifier_generator=lambda params_set: xgb.XGBClassifier(\n",
    "        max_depth=params_set[\"max_depth\"],\n",
    "        learning_rate=params_set[\"learning_rate\"],\n",
    "        n_estimators=params_set[\"rounds\"],\n",
    "        objective=params_set[\"objective\"],\n",
    "        tree_method=\"hist\",\n",
    "    ),\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=label_combinations,\n",
    "    name=\"XGBoost\",\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_combinations = dataset_manager.make_label_combinations(SELECTED_FEATURES)\n",
    "random_state = np.random.randint(low=0, high=100)\n",
    "params_sets = [\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"ball_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"ball_tree\", \"metric\": \"cityblock\"},\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"kd_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 5, \"algorithm\": \"kd_tree\", \"metric\": \"cityblock\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"ball_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"ball_tree\", \"metric\": \"cityblock\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"kd_tree\", \"metric\": \"minkowski\"},\n",
    "    {\"n_neighbors\": 3, \"algorithm\": \"kd_tree\", \"metric\": \"cityblock\"},\n",
    "]\n",
    "\n",
    "\n",
    "run(\n",
    "    dataset_manager=dataset_manager,\n",
    "    dataset_dataframe=dataset_dataframe,\n",
    "    classifier_generator=lambda params_set: neighbors.KNeighborsClassifier(\n",
    "        n_neighbors=params_set[\"n_neighbors\"],\n",
    "        algorithm=params_set[\"algorithm\"],\n",
    "        metric=params_set[\"metric\"],\n",
    "    ),\n",
    "    params_sets=params_sets,\n",
    "    label_combinations=label_combinations,\n",
    "    name=\"k-NN\",\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
